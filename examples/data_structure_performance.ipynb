{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent Data Structure Performance Analysis\n",
    "\n",
    "This notebook provides interactive analysis of Tier 3 core algorithm performance:\n",
    "- Skip List (lock-free vs locked)\n",
    "- BST (lock-free vs locked)\n",
    "- Stack (Treiber with elimination)\n",
    "\n",
    "Use this to evaluate which data structure best fits your workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Try to import real library, fall back to simulation\n",
    "try:\n",
    "    from concurrent_collections import SkipListMap, TreeMap, LockFreeStack\n",
    "    from concurrent_collections.profilers import SkipListProfiler, BSTProfiler, StackProfiler\n",
    "    SIMULATION_MODE = False\n",
    "    print(\"Using concurrent_collections library\")\n",
    "except ImportError:\n",
    "    SIMULATION_MODE = True\n",
    "    print(\"Library not installed - running in simulation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation classes for demonstration\n",
    "if SIMULATION_MODE:\n",
    "    @dataclass\n",
    "    class SimulatedStats:\n",
    "        total_inserts: int = 0\n",
    "        total_deletes: int = 0\n",
    "        total_searches: int = 0\n",
    "        cas_failure_rate: float = 0.0\n",
    "        insert_latency_p50: float = 0.0\n",
    "        insert_latency_p99: float = 0.0\n",
    "        search_latency_p50: float = 0.0\n",
    "        search_latency_p99: float = 0.0\n",
    "    \n",
    "    class SimulatedMap(dict):\n",
    "        def __init__(self, name=\"SimulatedMap\"):\n",
    "            super().__init__()\n",
    "            self.name = name\n",
    "            self._lock = threading.Lock()\n",
    "        \n",
    "        def put(self, key, value):\n",
    "            with self._lock:\n",
    "                self[key] = value\n",
    "        \n",
    "        def pop(self, key, default=None):\n",
    "            with self._lock:\n",
    "                return super().pop(key, default)\n",
    "    \n",
    "    class SimulatedStack:\n",
    "        def __init__(self):\n",
    "            self._items = []\n",
    "            self._lock = threading.Lock()\n",
    "        \n",
    "        def push(self, item):\n",
    "            with self._lock:\n",
    "                self._items.append(item)\n",
    "        \n",
    "        def pop(self):\n",
    "            with self._lock:\n",
    "                return self._items.pop() if self._items else None\n",
    "    \n",
    "    SkipListMap = lambda: SimulatedMap(\"SkipListMap\")\n",
    "    TreeMap = lambda: SimulatedMap(\"TreeMap\")\n",
    "    LockFreeStack = SimulatedStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    name: str\n",
    "    threads: int\n",
    "    ops_per_sec: float\n",
    "    latency_p50_us: float\n",
    "    latency_p99_us: float\n",
    "    total_ops: int\n",
    "    duration_sec: float\n",
    "\n",
    "def benchmark_map(map_factory: Callable, name: str, threads: int,\n",
    "                  ops_per_thread: int = 10000,\n",
    "                  read_ratio: float = 0.8) -> BenchmarkResult:\n",
    "    \"\"\"Benchmark a map implementation.\"\"\"\n",
    "    m = map_factory()\n",
    "    latencies = []\n",
    "    total_ops = [0]\n",
    "    \n",
    "    # Pre-populate\n",
    "    for i in range(1000):\n",
    "        m.put(i, i)\n",
    "    \n",
    "    def worker():\n",
    "        local_latencies = []\n",
    "        for _ in range(ops_per_thread):\n",
    "            key = random.randint(0, 2000)\n",
    "            start = time.perf_counter_ns()\n",
    "            \n",
    "            if random.random() < read_ratio:\n",
    "                _ = m.get(key)\n",
    "            elif random.random() < 0.5:\n",
    "                m.put(key, key)\n",
    "            else:\n",
    "                m.pop(key, None)\n",
    "            \n",
    "            elapsed = (time.perf_counter_ns() - start) / 1000\n",
    "            local_latencies.append(elapsed)\n",
    "        \n",
    "        with threading.Lock():\n",
    "            latencies.extend(local_latencies)\n",
    "            total_ops[0] += ops_per_thread\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        futures = [executor.submit(worker) for _ in range(threads)]\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    sorted_lat = sorted(latencies)\n",
    "    n = len(sorted_lat)\n",
    "    \n",
    "    return BenchmarkResult(\n",
    "        name=name,\n",
    "        threads=threads,\n",
    "        ops_per_sec=total_ops[0] / duration,\n",
    "        latency_p50_us=sorted_lat[int(n * 0.50)] if n > 0 else 0,\n",
    "        latency_p99_us=sorted_lat[min(int(n * 0.99), n-1)] if n > 0 else 0,\n",
    "        total_ops=total_ops[0],\n",
    "        duration_sec=duration\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip List vs BST Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Skip List and BST across thread counts\n",
    "thread_counts = [1, 2, 4, 8]\n",
    "skiplist_results = []\n",
    "tree_results = []\n",
    "\n",
    "print(\"Running benchmarks...\")\n",
    "for t in thread_counts:\n",
    "    print(f\"  {t} threads...\")\n",
    "    skiplist_results.append(benchmark_map(SkipListMap, \"SkipListMap\", t))\n",
    "    tree_results.append(benchmark_map(TreeMap, \"TreeMap\", t))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Throughput\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(thread_counts))\n",
    "width = 0.35\n",
    "\n",
    "skiplist_throughput = [r.ops_per_sec / 1000 for r in skiplist_results]\n",
    "tree_throughput = [r.ops_per_sec / 1000 for r in tree_results]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, skiplist_throughput, width, label='SkipListMap', color='steelblue')\n",
    "bars2 = ax1.bar(x + width/2, tree_throughput, width, label='TreeMap', color='coral')\n",
    "\n",
    "ax1.set_xlabel('Thread Count')\n",
    "ax1.set_ylabel('Throughput (K ops/sec)')\n",
    "ax1.set_title('Throughput Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(thread_counts)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Latency\n",
    "ax2 = axes[1]\n",
    "skiplist_p99 = [r.latency_p99_us for r in skiplist_results]\n",
    "tree_p99 = [r.latency_p99_us for r in tree_results]\n",
    "\n",
    "ax2.plot(thread_counts, skiplist_p99, 'o-', label='SkipListMap P99', color='steelblue')\n",
    "ax2.plot(thread_counts, tree_p99, 's-', label='TreeMap P99', color='coral')\n",
    "\n",
    "ax2.set_xlabel('Thread Count')\n",
    "ax2.set_ylabel('Latency (μs)')\n",
    "ax2.set_title('P99 Latency Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different read/write ratios\n",
    "read_ratios = [0.5, 0.8, 0.95, 0.99]\n",
    "skiplist_by_ratio = []\n",
    "tree_by_ratio = []\n",
    "\n",
    "print(\"Testing workload ratios (4 threads)...\")\n",
    "for ratio in read_ratios:\n",
    "    print(f\"  {int(ratio*100)}% reads...\")\n",
    "    skiplist_by_ratio.append(benchmark_map(SkipListMap, \"SkipList\", 4, read_ratio=ratio))\n",
    "    tree_by_ratio.append(benchmark_map(TreeMap, \"Tree\", 4, read_ratio=ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = [f\"{int(r*100)}%\" for r in read_ratios]\n",
    "skiplist_tp = [r.ops_per_sec / 1000 for r in skiplist_by_ratio]\n",
    "tree_tp = [r.ops_per_sec / 1000 for r in tree_by_ratio]\n",
    "\n",
    "x_pos = np.arange(len(read_ratios))\n",
    "ax.bar(x_pos - 0.2, skiplist_tp, 0.4, label='SkipListMap', color='steelblue')\n",
    "ax.bar(x_pos + 0.2, tree_tp, 0.4, label='TreeMap', color='coral')\n",
    "\n",
    "ax.set_xlabel('Read Ratio')\n",
    "ax.set_ylabel('Throughput (K ops/sec)')\n",
    "ax.set_title('Performance by Workload Type')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Performance with Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_stack(stack_factory: Callable, name: str, threads: int,\n",
    "                    ops_per_thread: int = 10000) -> BenchmarkResult:\n",
    "    \"\"\"Benchmark a stack implementation.\"\"\"\n",
    "    stack = stack_factory()\n",
    "    latencies = []\n",
    "    total_ops = [0]\n",
    "    \n",
    "    def worker():\n",
    "        local_latencies = []\n",
    "        for i in range(ops_per_thread):\n",
    "            start = time.perf_counter_ns()\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                stack.push(i)\n",
    "            else:\n",
    "                stack.pop()\n",
    "            \n",
    "            elapsed = (time.perf_counter_ns() - start) / 1000\n",
    "            local_latencies.append(elapsed)\n",
    "        \n",
    "        with threading.Lock():\n",
    "            latencies.extend(local_latencies)\n",
    "            total_ops[0] += ops_per_thread\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        futures = [executor.submit(worker) for _ in range(threads)]\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    sorted_lat = sorted(latencies)\n",
    "    n = len(sorted_lat)\n",
    "    \n",
    "    return BenchmarkResult(\n",
    "        name=name,\n",
    "        threads=threads,\n",
    "        ops_per_sec=total_ops[0] / duration,\n",
    "        latency_p50_us=sorted_lat[int(n * 0.50)] if n > 0 else 0,\n",
    "        latency_p99_us=sorted_lat[min(int(n * 0.99), n-1)] if n > 0 else 0,\n",
    "        total_ops=total_ops[0],\n",
    "        duration_sec=duration\n",
    "    )\n",
    "\n",
    "# Benchmark stack\n",
    "stack_results = []\n",
    "print(\"Benchmarking stack...\")\n",
    "for t in thread_counts:\n",
    "    print(f\"  {t} threads...\")\n",
    "    stack_results.append(benchmark_stack(LockFreeStack, \"LockFreeStack\", t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "stack_tp = [r.ops_per_sec / 1000 for r in stack_results]\n",
    "\n",
    "ax.bar(thread_counts, stack_tp, color='forestgreen', alpha=0.8)\n",
    "ax.set_xlabel('Thread Count')\n",
    "ax.set_ylabel('Throughput (K ops/sec)')\n",
    "ax.set_title('Lock-Free Stack Performance')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Annotate with P99 latency\n",
    "for i, (t, r) in enumerate(zip(thread_counts, stack_results)):\n",
    "    ax.annotate(f'P99: {r.latency_p99_us:.1f}μs',\n",
    "                xy=(t, stack_tp[i]), ha='center', va='bottom',\n",
    "                fontsize=9, color='darkgreen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(skiplist_results, tree_results, stack_results):\n",
    "    \"\"\"Generate data structure recommendations based on benchmark results.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Compare at highest thread count\n",
    "    sl = skiplist_results[-1]\n",
    "    tr = tree_results[-1]\n",
    "    st = stack_results[-1]\n",
    "    \n",
    "    # Ordered map recommendation\n",
    "    if sl.ops_per_sec > tr.ops_per_sec * 1.1:\n",
    "        recommendations.append({\n",
    "            'category': 'Ordered Map',\n",
    "            'recommendation': 'SkipListMap',\n",
    "            'reason': f'{sl.ops_per_sec/tr.ops_per_sec:.1f}x faster at {sl.threads} threads'\n",
    "        })\n",
    "    elif tr.ops_per_sec > sl.ops_per_sec * 1.1:\n",
    "        recommendations.append({\n",
    "            'category': 'Ordered Map',\n",
    "            'recommendation': 'TreeMap',\n",
    "            'reason': f'{tr.ops_per_sec/sl.ops_per_sec:.1f}x faster at {tr.threads} threads'\n",
    "        })\n",
    "    else:\n",
    "        recommendations.append({\n",
    "            'category': 'Ordered Map',\n",
    "            'recommendation': 'Either (similar performance)',\n",
    "            'reason': 'Choose based on range query needs (SkipList better for ranges)'\n",
    "        })\n",
    "    \n",
    "    # Latency analysis\n",
    "    if sl.latency_p99_us < tr.latency_p99_us:\n",
    "        recommendations.append({\n",
    "            'category': 'Latency-Sensitive',\n",
    "            'recommendation': 'SkipListMap',\n",
    "            'reason': f'P99 latency {sl.latency_p99_us:.1f}μs vs {tr.latency_p99_us:.1f}μs'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "recommendations = generate_recommendations(skiplist_results, tree_results, stack_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n{rec['category']}:\")\n",
    "    print(f\"  Recommended: {rec['recommendation']}\")\n",
    "    print(f\"  Reason: {rec['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBenchmark Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Data Structure':<20} {'Threads':>8} {'Throughput':>15} {'P50 (μs)':>12} {'P99 (μs)':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for r in skiplist_results:\n",
    "    print(f\"{r.name:<20} {r.threads:>8} {r.ops_per_sec/1000:>12.1f} K/s {r.latency_p50_us:>12.2f} {r.latency_p99_us:>12.2f}\")\n",
    "\n",
    "print()\n",
    "for r in tree_results:\n",
    "    print(f\"{r.name:<20} {r.threads:>8} {r.ops_per_sec/1000:>12.1f} K/s {r.latency_p50_us:>12.2f} {r.latency_p99_us:>12.2f}\")\n",
    "\n",
    "print()\n",
    "for r in stack_results:\n",
    "    print(f\"{r.name:<20} {r.threads:>8} {r.ops_per_sec/1000:>12.1f} K/s {r.latency_p50_us:>12.2f} {r.latency_p99_us:>12.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

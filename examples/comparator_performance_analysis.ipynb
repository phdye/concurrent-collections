{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Comparator Performance Analysis\n",
    "\n",
    "This notebook helps you evaluate whether your comparator is a performance bottleneck and provides data-driven recommendations for optimization.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Run the setup cells** to install dependencies and import modules\n",
    "2. **Define your comparator** in the \"Your Comparator\" section\n",
    "3. **Configure your workload** parameters\n",
    "4. **Run all cells** to get analysis and recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import time\n",
    "import random\n",
    "import statistics\n",
    "from typing import Callable, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    HAS_MATPLOTLIB = True\n",
    "except ImportError:\n",
    "    print(\"Installing matplotlib...\")\n",
    "    !pip install matplotlib -q\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    HAS_MATPLOTLIB = True\n",
    "\n",
    "# For nicer output\n",
    "try:\n",
    "    import pandas as pd\n",
    "    HAS_PANDAS = True\n",
    "except ImportError:\n",
    "    print(\"Installing pandas...\")\n",
    "    !pip install pandas -q\n",
    "    import pandas as pd\n",
    "    HAS_PANDAS = True\n",
    "\n",
    "# concurrent_collections (comment out if not installed yet)\n",
    "try:\n",
    "    from concurrent_collections import SkipListMap, Comparator\n",
    "    from concurrent_collections.instrumentation import ComparatorProfiler\n",
    "    HAS_CC = True\n",
    "except ImportError:\n",
    "    HAS_CC = False\n",
    "    print(\"âš ï¸  concurrent_collections not installed.\")\n",
    "    print(\"   This notebook will use simulation mode.\")\n",
    "    print(\"   Install with: pip install concurrent_collections\")\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulation-fallback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation mode fallback (when concurrent_collections not installed)\n",
    "# This simulates the profiler behavior for notebook demonstration\n",
    "\n",
    "if not HAS_CC:\n",
    "    @dataclass\n",
    "    class SimulatedReport:\n",
    "        wall_time_sec: float\n",
    "        comparator_time_sec: float\n",
    "        comparator_time_pct: float\n",
    "        comparison_count: int\n",
    "        avg_comparison_ns: float\n",
    "        p50_comparison_ns: float\n",
    "        p95_comparison_ns: float\n",
    "        p99_comparison_ns: float\n",
    "        p999_comparison_ns: float\n",
    "        max_comparison_ns: float\n",
    "        \n",
    "    class SimulatedProfiler:\n",
    "        def __init__(self, cmp_func, n_comparisons=100000):\n",
    "            self.cmp_func = cmp_func\n",
    "            self.n_comparisons = n_comparisons\n",
    "            self.latencies = []\n",
    "            self.report = None\n",
    "            \n",
    "        def __enter__(self):\n",
    "            return self\n",
    "            \n",
    "        def __exit__(self, *args):\n",
    "            pass\n",
    "            \n",
    "        def run_benchmark(self, test_data):\n",
    "            \"\"\"Run comparison benchmark and collect latencies.\"\"\"\n",
    "            self.latencies = []\n",
    "            \n",
    "            start_total = time.perf_counter()\n",
    "            cmp_time = 0\n",
    "            \n",
    "            for i in range(min(self.n_comparisons, len(test_data) - 1)):\n",
    "                a, b = test_data[i], test_data[(i + 1) % len(test_data)]\n",
    "                \n",
    "                start = time.perf_counter_ns()\n",
    "                _ = self.cmp_func(a, b)\n",
    "                elapsed_ns = time.perf_counter_ns() - start\n",
    "                \n",
    "                self.latencies.append(elapsed_ns)\n",
    "                cmp_time += elapsed_ns\n",
    "            \n",
    "            wall_time = time.perf_counter() - start_total\n",
    "            cmp_time_sec = cmp_time / 1e9\n",
    "            \n",
    "            sorted_lat = sorted(self.latencies)\n",
    "            n = len(sorted_lat)\n",
    "            \n",
    "            self.report = SimulatedReport(\n",
    "                wall_time_sec=wall_time,\n",
    "                comparator_time_sec=cmp_time_sec,\n",
    "                comparator_time_pct=(cmp_time_sec / wall_time * 100) if wall_time > 0 else 0,\n",
    "                comparison_count=len(self.latencies),\n",
    "                avg_comparison_ns=statistics.mean(self.latencies) if self.latencies else 0,\n",
    "                p50_comparison_ns=sorted_lat[int(n * 0.50)] if n > 0 else 0,\n",
    "                p95_comparison_ns=sorted_lat[int(n * 0.95)] if n > 0 else 0,\n",
    "                p99_comparison_ns=sorted_lat[int(n * 0.99)] if n > 0 else 0,\n",
    "                p999_comparison_ns=sorted_lat[int(n * 0.999)] if n > 0 else 0,\n",
    "                max_comparison_ns=max(self.latencies) if self.latencies else 0,\n",
    "            )\n",
    "            return self.report\n",
    "    \n",
    "    print(\"âœ… Simulation mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Your Comparator\n",
    "\n",
    "Define the comparator you want to evaluate. Modify the `my_comparator` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "user-comparator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# â¬‡ï¸ MODIFY THIS: Define your custom comparator\n",
    "# ============================================================\n",
    "\n",
    "def my_comparator(a, b) -> int:\n",
    "    \"\"\"\n",
    "    Your custom comparator function.\n",
    "    \n",
    "    Returns:\n",
    "        negative if a < b\n",
    "        zero if a == b  \n",
    "        positive if a > b\n",
    "    \"\"\"\n",
    "    # Example: Case-insensitive string comparison\n",
    "    a_lower = a.lower()\n",
    "    b_lower = b.lower()\n",
    "    return (a_lower > b_lower) - (a_lower < b_lower)\n",
    "\n",
    "# ============================================================\n",
    "# Baseline comparator (natural ordering) for comparison\n",
    "# ============================================================\n",
    "\n",
    "def natural_comparator(a, b) -> int:\n",
    "    \"\"\"Standard Python comparison.\"\"\"\n",
    "    return (a > b) - (a < b)\n",
    "\n",
    "print(\"âœ… Comparators defined\")\n",
    "print(f\"   my_comparator: {my_comparator.__doc__.strip().split(chr(10))[0] if my_comparator.__doc__ else 'Custom comparator'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workload-header",
   "metadata": {},
   "source": [
    "## 3. Workload Configuration\n",
    "\n",
    "Configure parameters that match your real-world usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workload-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# â¬‡ï¸ MODIFY THIS: Configure your workload\n",
    "# ============================================================\n",
    "\n",
    "# Data type you're comparing\n",
    "DATA_TYPE = \"string\"  # Options: \"string\", \"integer\", \"float\", \"custom\"\n",
    "\n",
    "# Number of items in your container (affects comparisons per operation)\n",
    "CONTAINER_SIZE = 100_000\n",
    "\n",
    "# Number of operations to benchmark\n",
    "N_OPERATIONS = 50_000\n",
    "\n",
    "# String length (if DATA_TYPE == \"string\")\n",
    "STRING_LENGTH = 20\n",
    "\n",
    "# Your expected operations per second in production\n",
    "EXPECTED_OPS_PER_SEC = 10_000\n",
    "\n",
    "# ============================================================\n",
    "# Generate test data\n",
    "# ============================================================\n",
    "\n",
    "def generate_test_data(data_type: str, n: int) -> list:\n",
    "    \"\"\"Generate test data based on type.\"\"\"\n",
    "    if data_type == \"string\":\n",
    "        chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        return [\"\".join(random.choices(chars, k=STRING_LENGTH)) for _ in range(n)]\n",
    "    elif data_type == \"integer\":\n",
    "        return [random.randint(-1_000_000, 1_000_000) for _ in range(n)]\n",
    "    elif data_type == \"float\":\n",
    "        return [random.uniform(-1000.0, 1000.0) for _ in range(n)]\n",
    "    else:\n",
    "        # Custom: override this for your data type\n",
    "        return list(range(n))\n",
    "\n",
    "test_data = generate_test_data(DATA_TYPE, max(CONTAINER_SIZE, N_OPERATIONS))\n",
    "print(f\"âœ… Generated {len(test_data):,} test items of type '{DATA_TYPE}'\")\n",
    "print(f\"   Sample: {test_data[0]!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "## 4. Run Benchmarks\n",
    "\n",
    "Compare your comparator against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmarks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark configuration\n",
    "N_COMPARISONS = min(N_OPERATIONS * 20, 500_000)  # ~log2(N) comparisons per op\n",
    "\n",
    "print(f\"Running {N_COMPARISONS:,} comparisons per comparator...\")\n",
    "print()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Benchmark each comparator\n",
    "comparators = {\n",
    "    \"Your Comparator\": my_comparator,\n",
    "    \"Natural (baseline)\": natural_comparator,\n",
    "}\n",
    "\n",
    "for name, cmp_func in comparators.items():\n",
    "    print(f\"Benchmarking: {name}...\")\n",
    "    \n",
    "    if HAS_CC:\n",
    "        # Use real profiler\n",
    "        # Note: This would require actual container operations\n",
    "        # For now, use simulation\n",
    "        profiler = SimulatedProfiler(cmp_func, N_COMPARISONS)\n",
    "    else:\n",
    "        profiler = SimulatedProfiler(cmp_func, N_COMPARISONS)\n",
    "    \n",
    "    report = profiler.run_benchmark(test_data)\n",
    "    results[name] = {\n",
    "        \"report\": report,\n",
    "        \"latencies\": profiler.latencies,\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ“ {report.comparison_count:,} comparisons in {report.wall_time_sec:.3f}s\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… Benchmarks complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "rows = []\n",
    "baseline_avg = results[\"Natural (baseline)\"][\"report\"].avg_comparison_ns\n",
    "\n",
    "for name, data in results.items():\n",
    "    r = data[\"report\"]\n",
    "    overhead = (r.avg_comparison_ns / baseline_avg - 1) * 100 if baseline_avg > 0 else 0\n",
    "    \n",
    "    rows.append({\n",
    "        \"Comparator\": name,\n",
    "        \"Avg (ns)\": f\"{r.avg_comparison_ns:,.0f}\",\n",
    "        \"P50 (ns)\": f\"{r.p50_comparison_ns:,.0f}\",\n",
    "        \"P95 (ns)\": f\"{r.p95_comparison_ns:,.0f}\",\n",
    "        \"P99 (ns)\": f\"{r.p99_comparison_ns:,.0f}\",\n",
    "        \"Max (ns)\": f\"{r.max_comparison_ns:,.0f}\",\n",
    "        \"Overhead\": f\"{overhead:+.0f}%\" if name != \"Natural (baseline)\" else \"baseline\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LATENCY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impact-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate production impact\n",
    "your_report = results[\"Your Comparator\"][\"report\"]\n",
    "baseline_report = results[\"Natural (baseline)\"][\"report\"]\n",
    "\n",
    "# Estimate comparisons per operation (log2 of container size)\n",
    "import math\n",
    "comparisons_per_op = int(math.log2(CONTAINER_SIZE)) + 5  # +5 for overhead\n",
    "\n",
    "# Time spent in comparisons per second at expected load\n",
    "comparisons_per_sec = EXPECTED_OPS_PER_SEC * comparisons_per_op\n",
    "your_cmp_time_per_sec = comparisons_per_sec * your_report.avg_comparison_ns / 1e9\n",
    "baseline_cmp_time_per_sec = comparisons_per_sec * baseline_report.avg_comparison_ns / 1e9\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION IMPACT ESTIMATE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nWorkload assumptions:\")\n",
    "print(f\"  Container size:     {CONTAINER_SIZE:,} items\")\n",
    "print(f\"  Operations/sec:     {EXPECTED_OPS_PER_SEC:,}\")\n",
    "print(f\"  Comparisons/op:     ~{comparisons_per_op} (logâ‚‚({CONTAINER_SIZE:,}))\")\n",
    "print(f\"  Comparisons/sec:    {comparisons_per_sec:,}\")\n",
    "print()\n",
    "print(f\"Time in comparisons per second:\")\n",
    "print(f\"  Your comparator:    {your_cmp_time_per_sec*1000:.2f} ms/sec ({your_cmp_time_per_sec*100:.1f}% of 1 CPU)\")\n",
    "print(f\"  Natural ordering:   {baseline_cmp_time_per_sec*1000:.2f} ms/sec ({baseline_cmp_time_per_sec*100:.1f}% of 1 CPU)\")\n",
    "print(f\"  Additional cost:    {(your_cmp_time_per_sec - baseline_cmp_time_per_sec)*1000:.2f} ms/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency-histogram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency distribution histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db']\n",
    "\n",
    "for idx, (name, data) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    latencies = data[\"latencies\"]\n",
    "    \n",
    "    # Cap at P99 for visualization\n",
    "    p99 = sorted(latencies)[int(len(latencies) * 0.99)]\n",
    "    display_latencies = [l for l in latencies if l <= p99 * 1.5]\n",
    "    \n",
    "    ax.hist(display_latencies, bins=50, color=colors[idx], alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(data[\"report\"].p50_comparison_ns, color='red', linestyle='--', \n",
    "               label=f'P50: {data[\"report\"].p50_comparison_ns:,.0f} ns')\n",
    "    ax.axvline(data[\"report\"].p95_comparison_ns, color='orange', linestyle='--',\n",
    "               label=f'P95: {data[\"report\"].p95_comparison_ns:,.0f} ns')\n",
    "    ax.axvline(data[\"report\"].p99_comparison_ns, color='purple', linestyle='--',\n",
    "               label=f'P99: {data[\"report\"].p99_comparison_ns:,.0f} ns')\n",
    "    \n",
    "    ax.set_xlabel('Latency (ns)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{name}\\nAvg: {data[\"report\"].avg_comparison_ns:,.0f} ns')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparison Latency Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "percentile-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "percentiles = ['P50', 'P95', 'P99', 'P99.9']\n",
    "x = range(len(percentiles))\n",
    "width = 0.35\n",
    "\n",
    "your_values = [\n",
    "    your_report.p50_comparison_ns,\n",
    "    your_report.p95_comparison_ns,\n",
    "    your_report.p99_comparison_ns,\n",
    "    your_report.p999_comparison_ns,\n",
    "]\n",
    "\n",
    "baseline_values = [\n",
    "    baseline_report.p50_comparison_ns,\n",
    "    baseline_report.p95_comparison_ns,\n",
    "    baseline_report.p99_comparison_ns,\n",
    "    baseline_report.p999_comparison_ns,\n",
    "]\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], your_values, width, label='Your Comparator', color='#3498db')\n",
    "bars2 = ax.bar([i + width/2 for i in x], baseline_values, width, label='Natural (baseline)', color='#2ecc71')\n",
    "\n",
    "ax.set_xlabel('Percentile')\n",
    "ax.set_ylabel('Latency (ns)')\n",
    "ax.set_title('Latency Percentile Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(percentiles)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:,.0f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:,.0f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tail-latency-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail latency analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Check for tail latency issues\n",
    "your_tail_ratio = your_report.p99_comparison_ns / your_report.p50_comparison_ns\n",
    "baseline_tail_ratio = baseline_report.p99_comparison_ns / baseline_report.p50_comparison_ns\n",
    "\n",
    "ratios = {\n",
    "    'Your Comparator': your_tail_ratio,\n",
    "    'Natural (baseline)': baseline_tail_ratio,\n",
    "}\n",
    "\n",
    "colors_tail = ['#e74c3c' if r > 10 else '#f39c12' if r > 5 else '#2ecc71' for r in ratios.values()]\n",
    "\n",
    "bars = ax.bar(ratios.keys(), ratios.values(), color=colors_tail, edgecolor='black')\n",
    "ax.axhline(y=5, color='orange', linestyle='--', label='Warning threshold (5x)')\n",
    "ax.axhline(y=10, color='red', linestyle='--', label='Critical threshold (10x)')\n",
    "\n",
    "ax.set_ylabel('P99 / P50 Ratio')\n",
    "ax.set_title('Tail Latency Ratio (P99 / P50)\\nHigher = more latency variability')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, ratio in zip(bars, ratios.values()):\n",
    "    ax.annotate(f'{ratio:.1f}x',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations-header",
   "metadata": {},
   "source": [
    "## 7. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "\n",
    "def generate_recommendations(your_report, baseline_report, comparisons_per_sec):\n",
    "    \"\"\"Generate actionable recommendations based on benchmark results.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    overhead_pct = (your_report.avg_comparison_ns / baseline_report.avg_comparison_ns - 1) * 100\n",
    "    overhead_ns = your_report.avg_comparison_ns - baseline_report.avg_comparison_ns\n",
    "    tail_ratio = your_report.p99_comparison_ns / your_report.p50_comparison_ns\n",
    "    time_per_sec = comparisons_per_sec * your_report.avg_comparison_ns / 1e9\n",
    "    \n",
    "    # Severity levels\n",
    "    severity = \"âœ… LOW\"\n",
    "    if time_per_sec > 0.3:  # >30% of 1 CPU\n",
    "        severity = \"ðŸ”´ CRITICAL\"\n",
    "    elif time_per_sec > 0.1:  # >10% of 1 CPU\n",
    "        severity = \"ðŸŸ  HIGH\"\n",
    "    elif time_per_sec > 0.05:  # >5% of 1 CPU\n",
    "        severity = \"ðŸŸ¡ MODERATE\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"RECOMMENDATIONS                                      Severity: {severity}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overhead analysis\n",
    "    print(f\"\\nðŸ“Š OVERHEAD ANALYSIS\")\n",
    "    print(f\"   Your comparator is {overhead_pct:+.0f}% vs natural ordering ({overhead_ns:+.0f} ns/comparison)\")\n",
    "    \n",
    "    if overhead_pct < 50:\n",
    "        print(f\"   âœ… Overhead is acceptable for most use cases\")\n",
    "    elif overhead_pct < 200:\n",
    "        print(f\"   âš ï¸  Moderate overhead - consider optimization if this is a hot path\")\n",
    "    else:\n",
    "        print(f\"   ðŸ”´ High overhead - strongly consider optimization\")\n",
    "    \n",
    "    # Tail latency analysis\n",
    "    print(f\"\\nðŸ“ˆ TAIL LATENCY ANALYSIS\")\n",
    "    print(f\"   P99/P50 ratio: {tail_ratio:.1f}x\")\n",
    "    \n",
    "    if tail_ratio < 3:\n",
    "        print(f\"   âœ… Consistent latency - no tail latency issues\")\n",
    "    elif tail_ratio < 10:\n",
    "        print(f\"   âš ï¸  Some latency variability - may be GC or contention\")\n",
    "    else:\n",
    "        print(f\"   ðŸ”´ High tail latency - investigate GC, locks, or I/O in comparator\")\n",
    "    \n",
    "    # Production impact\n",
    "    print(f\"\\nðŸ­ PRODUCTION IMPACT\")\n",
    "    print(f\"   At {EXPECTED_OPS_PER_SEC:,} ops/sec: {time_per_sec*100:.1f}% of 1 CPU in comparisons\")\n",
    "    \n",
    "    if time_per_sec < 0.01:\n",
    "        print(f\"   âœ… Negligible impact - no optimization needed\")\n",
    "    elif time_per_sec < 0.05:\n",
    "        print(f\"   âœ… Low impact - current implementation is fine\")\n",
    "    elif time_per_sec < 0.1:\n",
    "        print(f\"   ðŸŸ¡ Moderate impact - consider key function if possible\")\n",
    "    elif time_per_sec < 0.3:\n",
    "        print(f\"   ðŸŸ  Significant impact - recommend native comparator\")\n",
    "    else:\n",
    "        print(f\"   ðŸ”´ Critical impact - native comparator strongly recommended\")\n",
    "    \n",
    "    # Specific recommendations\n",
    "    print(f\"\\nðŸ’¡ SPECIFIC RECOMMENDATIONS\")\n",
    "    \n",
    "    if time_per_sec < 0.05:\n",
    "        print(f\"   1. Keep your current Python comparator\")\n",
    "        print(f\"   2. No optimization needed at this scale\")\n",
    "    elif overhead_pct > 100:\n",
    "        print(f\"   1. Consider using key= instead of cmp= if possible\")\n",
    "        print(f\"      Example: SkipListMap(key=str.lower) instead of cmp=...\")\n",
    "        print(f\"   2. Key functions extract once, compare many times\")\n",
    "    \n",
    "    if time_per_sec > 0.1:\n",
    "        print(f\"   3. For maximum performance, implement a native comparator:\")\n",
    "        print(f\"      - Cython: Best for Python-focused projects\")\n",
    "        print(f\"      - Rust with C ABI: Best for new code\")\n",
    "        print(f\"      - C: Best for existing C codebases\")\n",
    "    \n",
    "    if tail_ratio > 5:\n",
    "        print(f\"   4. Investigate tail latency:\")\n",
    "        print(f\"      - Check for GC pressure in comparator\")\n",
    "        print(f\"      - Avoid creating temporary objects\")\n",
    "        print(f\"      - Consider string interning for repeated comparisons\")\n",
    "    \n",
    "    # Estimated improvement\n",
    "    print(f\"\\nðŸ“‰ ESTIMATED IMPROVEMENT WITH NATIVE COMPARATOR\")\n",
    "    native_estimate_ns = baseline_report.avg_comparison_ns * 1.2  # ~20% overhead for native\n",
    "    native_time_per_sec = comparisons_per_sec * native_estimate_ns / 1e9\n",
    "    improvement = (time_per_sec - native_time_per_sec) / time_per_sec * 100 if time_per_sec > 0 else 0\n",
    "    \n",
    "    print(f\"   Current:  {time_per_sec*100:.1f}% CPU\")\n",
    "    print(f\"   Estimated: {native_time_per_sec*100:.1f}% CPU (native)\")\n",
    "    print(f\"   Potential improvement: ~{improvement:.0f}%\")\n",
    "    \n",
    "    if improvement > 50:\n",
    "        print(f\"   âœ… Native comparator would be worthwhile\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Limited benefit from native comparator at this scale\")\n",
    "\n",
    "generate_recommendations(your_report, baseline_report, comparisons_per_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results for sharing\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "export_data = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"configuration\": {\n",
    "        \"data_type\": DATA_TYPE,\n",
    "        \"container_size\": CONTAINER_SIZE,\n",
    "        \"n_operations\": N_OPERATIONS,\n",
    "        \"expected_ops_per_sec\": EXPECTED_OPS_PER_SEC,\n",
    "    },\n",
    "    \"results\": {}\n",
    "}\n",
    "\n",
    "for name, data in results.items():\n",
    "    r = data[\"report\"]\n",
    "    export_data[\"results\"][name] = {\n",
    "        \"avg_ns\": r.avg_comparison_ns,\n",
    "        \"p50_ns\": r.p50_comparison_ns,\n",
    "        \"p95_ns\": r.p95_comparison_ns,\n",
    "        \"p99_ns\": r.p99_comparison_ns,\n",
    "        \"p999_ns\": r.p999_comparison_ns,\n",
    "        \"max_ns\": r.max_comparison_ns,\n",
    "        \"comparison_count\": r.comparison_count,\n",
    "    }\n",
    "\n",
    "# Save to file\n",
    "output_file = \"comparator_benchmark_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Results exported to {output_file}\")\n",
    "print()\n",
    "print(\"JSON preview:\")\n",
    "print(json.dumps(export_data, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-header",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "Based on the recommendations above:\n",
    "\n",
    "### If optimization is NOT needed:\n",
    "- Keep your current Python comparator\n",
    "- Re-run this analysis if your workload changes significantly\n",
    "\n",
    "### If key function is recommended:\n",
    "```python\n",
    "# Instead of:\n",
    "m = SkipListMap(cmp=lambda a, b: (a.lower() > b.lower()) - (a.lower() < b.lower()))\n",
    "\n",
    "# Use:\n",
    "m = SkipListMap(key=str.lower)\n",
    "```\n",
    "\n",
    "### If native comparator is recommended:\n",
    "\n",
    "See the [Comparator Design Documentation](../doc/design/tier-1/comparator/design.md) for:\n",
    "- Cython implementation examples\n",
    "- Rust with C ABI examples\n",
    "- Pure C examples\n",
    "\n",
    "### Re-run after optimization:\n",
    "1. Implement your native comparator\n",
    "2. Update the `my_comparator` cell to use it\n",
    "3. Re-run all cells to verify improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "footer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Summary:\")\n",
    "print(f\"  Your comparator:    {your_report.avg_comparison_ns:,.0f} ns avg\")\n",
    "print(f\"  Natural ordering:   {baseline_report.avg_comparison_ns:,.0f} ns avg\")\n",
    "print(f\"  Overhead:           {(your_report.avg_comparison_ns / baseline_report.avg_comparison_ns - 1) * 100:+.0f}%\")\n",
    "print(f\"  Production impact:  {comparisons_per_sec * your_report.avg_comparison_ns / 1e9 * 100:.1f}% CPU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
